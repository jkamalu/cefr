{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Setup\n",
    "\n",
    "Citation: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNALE_PATH = '../data/ICNALE'\n",
    "TEST_VAL_SPLIT = 0.8\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gen():\n",
    "    class_map = {\n",
    "        'A2-0': 0,\n",
    "        'B1-1': 1,\n",
    "        'B1-2': 2,\n",
    "        'B2-0': 3\n",
    "    }\n",
    "    \n",
    "    for path in sorted(os.listdir(ICNALE_PATH)):\n",
    "        file_name, file_ext = path.split('.')\n",
    "        if file_name == 'W_CHN_SMK_B1_1': continue\n",
    "        level = '-'.join(file_name.split('_')[3:])\n",
    "        if level in class_map:\n",
    "            level = class_map[level]\n",
    "        else:\n",
    "            level = len(class_map)\n",
    "        with open('{}/{}'.format(ICNALE_PATH, path), 'r') as fd:\n",
    "            for sample in fd:\n",
    "                sample = sample.decode(\"utf-8-sig\")\n",
    "                sample = sample.strip('\\n')\n",
    "                sample = sample.strip('\\r')\n",
    "                if sample == '': continue\n",
    "                yield sample, level\n",
    "                \n",
    "def to_sentences(text):\n",
    "    stop = re.compile(r'([\\.?!])')\n",
    "    sentences = []\n",
    "    for split in stop.split(text):\n",
    "        if split == '': continue\n",
    "        if stop.match(split):\n",
    "            sentences[-1] = sentences[-1] + split\n",
    "        else:\n",
    "            sentences.append(split.strip())\n",
    "    return sentences\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = re.sub(r'([^A-z0-9\\s])', r' \\1', sentence)\n",
    "    return text_to_word_sequence(sentence, filters='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 5418 samples\n",
      "Sample length covering %80 of samples = 286\n"
     ]
    }
   ],
   "source": [
    "sampler = sample_gen()\n",
    "\n",
    "texts = []\n",
    "levels = []\n",
    "\n",
    "sequences = []\n",
    "word_index = {}\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        sample = sampler.next()\n",
    "    except:\n",
    "        break\n",
    "    text, level = sample\n",
    "    texts.append(text)\n",
    "    levels.append(level)\n",
    "    \n",
    "    sequence = []    \n",
    "    tokens = tokenize(text)\n",
    "    for word in tokens:\n",
    "        if word not in word_index:\n",
    "            word_index[word] = len(word_index) + 1\n",
    "        sequence.append(word_index[word])\n",
    "    sequences.append(sequence)\n",
    "    \n",
    "lengths = sorted([len(seq) for seq in sequences])\n",
    "coverage = 80\n",
    "coverage_length = lengths[len(lengths) * coverage // 100]\n",
    "    \n",
    "print('Parsed {} samples'.format(len(texts)))\n",
    "print('Sample length covering %{} of samples = {}'.format(coverage, coverage_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (5418, 286)\n",
      "Shape of labels: (5418, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=coverage_length, padding='post')\n",
    "labels = to_categorical(levels)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "with open('../data/data.pkl', 'w+') as fd:\n",
    "    pkl.dump(data, fd)\n",
    "with open('../data/labels.pkl', 'w+') as fd:\n",
    "    pkl.dump(labels, fd)\n",
    "\n",
    "print('Shape of data: {}'.format(data.shape))\n",
    "print('Shape of labels: {}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('../data/glove.6B.50d.txt') as fd:\n",
    "    for line in fd:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "with open('../data/embed_matrix.pkl', 'w+') as fd:\n",
    "    pkl.dump(embedding_matrix, fd)\n",
    "        \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
