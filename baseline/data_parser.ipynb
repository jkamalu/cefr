{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from featurizer import LexicalFeaturizer\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# import tensorflow.contrib.learn as skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ICNALE_Written_Essays_2.3'\n",
    "merged_plain_dir = '{}/Merged/Plain Text'.format(data_dir)\n",
    "merged_tagged_dir = '{}/Merged/Tagged'.format(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_mapping = {\n",
    "    'A2': 3,\n",
    "    'B1': 2,\n",
    "    'B2': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict = Counter()\n",
    "unigram_POS = Counter()\n",
    "\n",
    "bigram_dict = Counter()\n",
    "bigram_POS = Counter()\n",
    "\n",
    "script_length_dict = {}   #should have actual and POS one (POS adds an extra length whenever a sentence ends or grammar)\n",
    "\n",
    "punct_regex = re.compile(\"[/.!,?\\s]\")  # end of sentence\n",
    "grammar_regex = re.compile(\"[,]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_merged_plain_v1():\n",
    "    data = []\n",
    "    labels = []\n",
    "    featurizer = LexicalFeaturizer()\n",
    "    \n",
    "    # Begins reading the merged plain file\n",
    "    for path in sorted(os.listdir(merged_plain_dir)):\n",
    "        file_name, file_ext = path.split('.')\n",
    "        attributes = file_name.split('_')\n",
    "\n",
    "        if attributes[3] in level_mapping:\n",
    "            level = level_mapping[attributes[3]]\n",
    "        else: \n",
    "            level = 0\n",
    "            \n",
    "#         if level == 1: print(path)\n",
    "        sample_counter = 0\n",
    "        sample_avg_words = 0\n",
    "        with open('{}/{}'.format(merged_plain_dir, path), 'r', encoding='utf-8-sig') as file:\n",
    "            for sample in file:\n",
    "                if sample == '\\n': continue\n",
    "                sample = sample.strip('\\n')\n",
    "                \n",
    "                sample_words = sample.split()\n",
    "                paragraph_len = len(sample_words)\n",
    "                sample_avg_words += paragraph_len\n",
    "                paragraph_gram_len = len(sample_words) + 1\n",
    "                for i in range(paragraph_len):\n",
    "                    cur_word = sample_words[i].lower()\n",
    "                    cur_word = [ w for w in punct_regex.split(cur_word) if w]\n",
    "                    if len(cur_word) <= 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        cur_word = cur_word[0]\n",
    "#                     if punct_regex.search(cur_word):\n",
    "#                         paragraph_gram_len += 3\n",
    "#                     elif grammar_regex.search(cur_word):\n",
    "#                         paragraph_gram_len += 1\n",
    "                    unigram_dict[cur_word] += 1\n",
    "                    \n",
    "                    if i == 0:\n",
    "                        bigram = \"<s>\"\n",
    "                    else:\n",
    "                        bigram = sample_words[i - 1].lower()\n",
    "                    bigram += \" \" + cur_word\n",
    "                    bigram_dict[bigram] += 1\n",
    "                    if i == paragraph_len - 1:\n",
    "                        final_bigram = cur_word + \" </s>\"\n",
    "                        bigram_dict[final_bigram] += 1\n",
    "\n",
    "                script_length_dict[file_name+str(sample_counter)] = (paragraph_len, paragraph_gram_len)\n",
    "                sample_counter += 1\n",
    "#         print(sample_counter)\n",
    "#         print(sample_avg_words)\n",
    "#         print(sample_avg_words/sample_counter)\n",
    "\n",
    "        with open('{}/{}'.format(merged_plain_dir, path), 'r', encoding='utf-8-sig') as file:\n",
    "            for sample in file:\n",
    "                if sample == '\\n': continue\n",
    "                sample = sample.strip('\\n')\n",
    "                \n",
    "                p_features = featurizer.featurize(sample)\n",
    "                word_features = []\n",
    "                sample_words = sample.split()\n",
    "                paragraph_len = len(sample_words)\n",
    "                words = [ w for w in punct_regex.split(sample) if w]\n",
    "                most_common = Counter(words).most_common(20)\n",
    "                for i in range(len(most_common)):\n",
    "                    word, count = most_common[i]\n",
    "                    denom = unigram_dict[word.lower()]\n",
    "                    if denom == 0:\n",
    "                        print(\"\\n<<<<<<<-----------------\")\n",
    "                        print(unigram_dict[word.lower()])\n",
    "                        print(word.lower())\n",
    "                        word_features.append(1)\n",
    "                        print(\"\\n\\n\")\n",
    "                    else:\n",
    "                        word_features.append(count/denom + 1)\n",
    "                data.append(np.array(p_features))# + p_features))  #TODO: add avg_sent_len and number of sentence))\n",
    "                labels.append(level)\n",
    "\n",
    "            \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scorer = make_scorer(f1_score, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethson/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data, labels = parse_merged_plain_v1()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "# parameters = {'max_depth': range(10,50, 10), 'splitter': ('best', 'random')}\n",
    "# clf = GridSearchCV(LogisticRegression(), parameters, scoring=f1_scorer)\n",
    "clf = LogisticRegression(solver=\"saga\", multi_class=\"multinomial\", max_iter=100, verbose=1)\n",
    "# clf = tree.DecisionTreeClassifier(max_depth=8)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    0  276    0]\n",
      " [   0    0  299    0]\n",
      " [   0    0 2378    1]\n",
      " [   0    0  675    0]]\n",
      "0.19973430278583854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethson/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# y_true = labels\n",
    "# cnt = Counter(labels)\n",
    "# print(cnt)\n",
    "# print(clf.get_params(deep=False))\n",
    "y_pred_train = clf.predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "# print(clf.best_params_)\n",
    "print(f1_score(y_train, y_pred_train, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3    0  120    0]\n",
      " [   0    0  165    0]\n",
      " [   1    0 1165    0]\n",
      " [   0    0  334    1]]\n",
      "0.21075674597878657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethson/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# y_true = labels\n",
    "# cnt = Counter(labels)\n",
    "# print(cnt)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([featurizer.featurize(\"I agree that it is important for college students to have apart time job. Nowadays, a large number of college students are having a part time job. Some of them hold that part time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part time to earn enough money what a computer needs. Thus, it is important for college students to have a part time job. However, our parents always don't agree us to get part time jobs. They are afraid that our study and safety. In my opinion, having a good part time job is good for students. We should pay attention to the advantages of part time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part time jobs and make them a helpful tool for our development.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"In my opinion, I am strongly agree with the idea that, it is important for our college students to have a part-time job. Now we can see everywhere that student take part –jobs like private teacher, seller and so on. So why they do these jobs? What advantages benefit us? I think the main reason is money. With the improvement of our living standards, a lot of study material are expensive than before. Then the student has to do some jobs to reduce considerable financial pressure. By doing so, we can also have the ability to travel or buy some items we like. We also hope that through this way, we can no longer dependent on our parents. In addition, we can also accumulate some social experience. From kindergarten to high school, what we learned is totally the knowledge from books. In this way, it does a lot benefits to our future jobs. In a word, there are many advantages for our college students to do some jobs. Not only make money, but also develop our independence. So if you a lot of free time after class, try to look for a part-time job and you will not regret about it.\"\n",
    "sampleb2 = \"Whether college students should take part-time jobs has aroused great public concern. As far as I'm concerned, part-time job plays an increasing important role in college life and I'm for the idea. Nevertheless, several people argue that it is a waste of time to take a part-time job and obviously, college students ought to put more emphasis on their study. In addition, part-time job not only takes up much of their free time but also means little to them. On the contrary, others think that it is acceptable, reasonable, and beneficial to take a part-time job. The reasons are as follows. First of all, it is undeniable that college students can accumulate certain experience through part-time jobs. Then, as a result of doing part-time jobs, we can earn some pocket money in order to reduce parents' burden. What's more, admittedly, if you do some more work at your free time, you will find life is not so dull and you can do something meaningful to enjoy your college life. In a word, from my perspective, part-time job, to some extent, is indispensable to our college life. Regardless of other people's ideas, I support my own opinion and I think it is essential for college students to take part-time job.\"\n",
    "beginner = \"My family is big family. I have one elder sister and two younger sister. My elder sister is nurse. She very friendly. She have a one’s child. He name is Jong Youn. Jong Youn is very cute, so we are very happy. Jong Youn is very smiling. He looks good. My one’s younger sister draw very well. She’s name is Su Jeong. Su Jeong is small body. She dream is artist so she is everyday draw. My one’s younger sister name is Suhyun. She is 16 years old. She is student. So everyday study. My father is very busy. My mother too. My mother everyday cooking. Sometimes, my father help them. I have a my husband.\"\n",
    "intermediate = \"My home country is Japan, and I was born Tokyo in Japan. Tokyo is a capital city in Japan, so there are many people live in and enjoy their life. I’m Japanese, but I don’t know much about Japan, because I hate history, so I talk about the capital city of Tokyo. My hometown was a small country city, and there were nothing except for a small market and a convenience store. Also, the convenience store was far from my house, it took 30 minutes walk by myself. My friends and me were always played our school ground or a park that near from our house. I think, Tokyo is very famous for have a many entertainments. However, that is only center city of Tokyo. When I tell people who is from other countries, about my countries, they said Tokyo!! I know Tokyo. Although, that is totally different that their image or thought of Tokyo.\"\n",
    "advanced = \"I don’t usually drive to the campus, but the other day I woke up really late and I was going to miss my class. I took my morning shower, put on my clothes in five minutes and jumped into my car. After three minutes I arrived to the parking lot next to the Butler building. As expected, it was totally jammed. After circulating the area for more than ten times I managed to squeeze my car between a Mustang and a truck. After class I wanted to go out for a break. I started backing of the parking space looking to left. I just did not want to scratch that beautiful Mustang on my left side. And while I’m staring at it. All of a sudden I heard a crack sound. I looked to right to see that my right side mirror was totally in the truck taillight. I panicked for a moment. That was my first accident.\"\n",
    "unique = \"In my opinion, I am strongly agree with the idea that,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = LexicalFeaturizer()\n",
    "featurizer.featurize(beginner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurize(intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurize(advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurize(sampleb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurize(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "pattern = re.compile('([^\\s\\w]|_)+')\n",
    "print(re.sub(pattern, '', unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(\"I a b c d e g h i j k l m n o p q r s t u v w z x\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
