{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from featurizer import LexicalFeaturizer\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ICNALE_Written_Essays_2.3'\n",
    "merged_plain_dir = '{}/Merged/Plain Text'.format(data_dir)\n",
    "merged_tagged_dir = '{}/Merged/Tagged'.format(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_mapping = {\n",
    "    'A2_0': 4,\n",
    "    'B1_1': 3,\n",
    "    'B1_2': 2,\n",
    "    'B2_0': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_regex = re.compile(\"[/.!,?\\s]\")\n",
    "grammar_regex = re.compile(\"[,]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_merged_plain_v1():\n",
    "    script_length_dict = {} \n",
    "    \n",
    "    unigram_dict = Counter()\n",
    "    unigram_POS = Counter()\n",
    "\n",
    "    bigram_dict = Counter()\n",
    "    bigram_POS = Counter()    \n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    featurizer = LexicalFeaturizer()\n",
    "    \n",
    "    # Begins reading the merged plain file\n",
    "    for path in sorted(os.listdir(merged_plain_dir)):\n",
    "        file_name, file_ext = path.split('.')\n",
    "        attributes = file_name.split('_')\n",
    "\n",
    "        if len(attributes) == 4:\n",
    "            level = 0\n",
    "        else:\n",
    "            level = level_mapping['{}_{}'.format(attributes[3], attributes[4])]\n",
    "            \n",
    "        sample_counter = 0\n",
    "        sample_avg_words = 0\n",
    "        with open('{}/{}'.format(merged_plain_dir, path), 'r', encoding='utf-8-sig') as file:\n",
    "            for sample in file:\n",
    "                if sample == '\\n': continue\n",
    "                sample = sample.strip('\\n')\n",
    "                \n",
    "                sample_words = sample.split()\n",
    "                paragraph_len = len(sample_words)\n",
    "                sample_avg_words += paragraph_len\n",
    "                paragraph_gram_len = len(sample_words) + 1\n",
    "                for i in range(paragraph_len):\n",
    "                    cur_word = sample_words[i].lower()\n",
    "                    cur_word = [ w for w in punct_regex.split(cur_word) if w]\n",
    "                    if len(cur_word) <= 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        cur_word = cur_word[0]\n",
    "                    unigram_dict[cur_word] += 1\n",
    "                    \n",
    "                    if i == 0:\n",
    "                        bigram = \"<s>\"\n",
    "                    else:\n",
    "                        bigram = sample_words[i - 1].lower()\n",
    "                    bigram += \" \" + cur_word\n",
    "                    bigram_dict[bigram] += 1\n",
    "                    if i == paragraph_len - 1:\n",
    "                        final_bigram = cur_word + \" </s>\"\n",
    "                        bigram_dict[final_bigram] += 1\n",
    "\n",
    "                script_length_dict[file_name+str(sample_counter)] = (paragraph_len, paragraph_gram_len)\n",
    "                sample_counter += 1\n",
    "\n",
    "        with open('{}/{}'.format(merged_plain_dir, path), 'r', encoding='utf-8-sig') as file:\n",
    "            for sample in file:\n",
    "                if sample == '\\n': continue\n",
    "                sample = sample.strip('\\n')\n",
    "\n",
    "                p_features = featurizer.featurize(sample)\n",
    "                word_features = []\n",
    "                sample_words = sample.split()\n",
    "                words = [ w for w in punct_regex.split(sample) if w]\n",
    "                paragraph_len = len(words)\n",
    "                most_common = Counter(words).most_common(200)\n",
    "                for i in range(paragraph_len):\n",
    "                    cur_word = words[i]\n",
    "                    count = unigram_dict[cur_word.lower()]\n",
    "                    if count != 0:\n",
    "                        word_features.append(count/100)\n",
    "                data.append(np.array(p_features + word_features[:150])) #TODO: add avg_sent_len and number of sentence))\n",
    "                labels.append(level)\n",
    "    return data, labels\n",
    "data, labels = parse_merged_plain_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_label = {}\n",
    "for i in range(len(data)):\n",
    "    if labels[i] not in data_by_label:\n",
    "        data_by_label[labels[i]] = [data[i]]\n",
    "    else:\n",
    "        data_by_label[labels[i]].append(data[i])\n",
    "trimmed_data = []\n",
    "trimmed_labels = []\n",
    "for label in data_by_label:\n",
    "    trimmed_data.extend(data_by_label[label][:400])\n",
    "    trimmed_labels.extend([label for i in range(400)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            trimmed_data, trimmed_labels, test_size=0.20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logreg = LogisticRegression(solver=\"saga\", \n",
    "                         multi_class=\"multinomial\", \n",
    "                         max_iter=1000,\n",
    "                         verbose=1)\n",
    "clf_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names = ['Native', 'B2_0', 'B1_2', 'B1_1', 'A2_0']\n",
    "\n",
    "print('TRAIN')\n",
    "y_pred_train = clf_logreg.predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "scores = f1_score(y_train, y_pred_train, average=None)\n",
    "\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_train, y_pred_train, average='macro')))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('VALIDATION')\n",
    "y_pred = clf_logreg.predict(X_val)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "scores = f1_score(y_val, y_pred, average=None)\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_val, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST')\n",
    "y_pred = clf_logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('F1: {}'.format(f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dectree = tree.DecisionTreeClassifier(max_depth=10)\n",
    "clf_dectree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names = ['Native', 'B2_0', 'B1_2', 'B1_1', 'A2_0']\n",
    "\n",
    "print('TRAIN')\n",
    "y_pred_train = clf_dectree.predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "scores = f1_score(y_train, y_pred_train, average=None)\n",
    "\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_train, y_pred_train, average='macro')))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('VALIDATION')\n",
    "y_pred = clf_dectree.predict(X_val)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "scores = f1_score(y_val, y_pred, average=None)\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_val, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST')\n",
    "y_pred = clf_dectree.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = f1_score(y_test, y_pred, average=None)\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_test, y_pred, average='macro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
