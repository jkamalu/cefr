{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, re\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from featurizer import LexicalFeaturizer\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(texts, low, high):\n",
    "    n_vectorizer = CountVectorizer(ngram_range=(low, high))\n",
    "    counts = n_vectorizer.fit_transform(texts)\n",
    "    print n_vectorizer.get_feature_names()\n",
    "    return counts.toarray().astype(int)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'dog', u'dog dog', u'dog went', u'on', u'went', u'went on']\n",
      "[u'by', u'dog', u'dog went', u'went', u'went by']\n",
      "[u'by', u'dog', u'dog dog', u'dog went', u'on', u'went', u'went by', u'went on']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 2, 1, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams(['dog dog went on'], 1, 2)\n",
    "n_grams(['dog went by'], 1, 2)\n",
    "n_grams(['dog went by', 'dog dog went on'], 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ICNALE_Written_Essays_2.3'\n",
    "merged_plain_dir = '{}/Merged/Plain Text'.format(data_dir)\n",
    "merged_tagged_dir = '{}/Merged/Tagged'.format(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_mapping = {\n",
    "    'A2_0': 4,\n",
    "    'B1_1': 3,\n",
    "    'B1_2': 2,\n",
    "    'B2_0': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_regex = re.compile(\"[/.!,?\\s]\")\n",
    "grammar_regex = re.compile(\"[,]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../data/common_freq.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-173f6c8eba42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_merged_plain_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-173f6c8eba42>\u001b[0m in \u001b[0;36mparse_merged_plain_v1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfeaturizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLexicalFeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Begins reading the merged plain file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/john_kamalu/cefr/baseline/featurizer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcrete_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_freq_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/john_kamalu/cefr/baseline/featurizer.pyc\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_freq_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_concreteness_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/john_kamalu/cefr/baseline/featurizer.pyc\u001b[0m in \u001b[0;36mread_freq_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_freq_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/common_freq.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../data/common_freq.csv'"
     ]
    }
   ],
   "source": [
    "def parse_merged_plain_v1():\n",
    "    script_length_dict = {} \n",
    "    \n",
    "    unigram_dict = Counter()\n",
    "    unigram_POS = Counter()\n",
    "\n",
    "    bigram_dict = Counter()\n",
    "    bigram_POS = Counter()    \n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    featurizer = LexicalFeaturizer()\n",
    "    \n",
    "    # Begins reading the merged plain file\n",
    "    for path in sorted(os.listdir(merged_plain_dir)):\n",
    "        file_name, file_ext = path.split('.')\n",
    "        attributes = file_name.split('_')\n",
    "\n",
    "        if len(attributes) == 4:\n",
    "            level = 0\n",
    "        else:\n",
    "            level = level_mapping['{}_{}'.format(attributes[3], attributes[4])]\n",
    "            \n",
    "        sample_counter = 0\n",
    "        sample_avg_words = 0\n",
    "        with open('{}/{}'.format(merged_plain_dir, path), 'r', encoding='utf-8-sig') as file:\n",
    "            for sample in file:\n",
    "                if sample == '\\n': continue\n",
    "                sample = sample.strip('\\n')\n",
    "                \n",
    "                sample_words = sample.split()\n",
    "                paragraph_len = len(sample_words)\n",
    "                sample_avg_words += paragraph_len\n",
    "                paragraph_gram_len = len(sample_words) + 1\n",
    "                for i in range(paragraph_len):\n",
    "                    cur_word = sample_words[i].lower()\n",
    "                    cur_word = [ w for w in punct_regex.split(cur_word) if w]\n",
    "                    if len(cur_word) <= 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        cur_word = cur_word[0]\n",
    "                    unigram_dict[cur_word] += 1\n",
    "                    \n",
    "                    if i == 0:\n",
    "                        bigram = \"<s>\"\n",
    "                    else:\n",
    "                        bigram = sample_words[i - 1].lower()\n",
    "                    bigram += \" \" + cur_word\n",
    "                    bigram_dict[bigram] += 1\n",
    "                    if i == paragraph_len - 1:\n",
    "                        final_bigram = cur_word + \" </s>\"\n",
    "                        bigram_dict[final_bigram] += 1\n",
    "\n",
    "                script_length_dict[file_name+str(sample_counter)] = (paragraph_len, paragraph_gram_len)\n",
    "                sample_counter += 1\n",
    "\n",
    "        with open('{}/{}'.format(merged_plain_dir, path), 'r', encoding='utf-8-sig') as file:\n",
    "            for sample in file:\n",
    "                if sample == '\\n': continue\n",
    "                sample = sample.strip('\\n')\n",
    "\n",
    "                p_features = featurizer.featurize(sample)\n",
    "                word_features = []\n",
    "                sample_words = sample.split()\n",
    "                words = [ w for w in punct_regex.split(sample) if w]\n",
    "                paragraph_len = len(words)\n",
    "                most_common = Counter(words).most_common(200)\n",
    "                for i in range(paragraph_len):\n",
    "                    cur_word = words[i]\n",
    "                    count = unigram_dict[cur_word.lower()]\n",
    "                    if count != 0:\n",
    "                        word_features.append(count/100)\n",
    "                data.append(np.array(p_features + word_features[:150])) #TODO: add avg_sent_len and number of sentence))\n",
    "                labels.append(level)\n",
    "    return data, labels\n",
    "data, labels = parse_merged_plain_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_label = {}\n",
    "for i in range(len(data)):\n",
    "    if labels[i] not in data_by_label:\n",
    "        data_by_label[labels[i]] = [data[i]]\n",
    "    else:\n",
    "        data_by_label[labels[i]].append(data[i])\n",
    "trimmed_data = []\n",
    "trimmed_labels = []\n",
    "for label in data_by_label:\n",
    "    trimmed_data.extend(data_by_label[label][:400])\n",
    "    trimmed_labels.extend([label for i in range(400)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            trimmed_data, trimmed_labels, test_size=0.20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logreg = LogisticRegression(solver=\"saga\", \n",
    "                         multi_class=\"multinomial\", \n",
    "                         max_iter=1000,\n",
    "                         verbose=1)\n",
    "clf_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names = ['Native', 'B2_0', 'B1_2', 'B1_1', 'A2_0']\n",
    "\n",
    "print('TRAIN')\n",
    "y_pred_train = clf_logreg.predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "scores = f1_score(y_train, y_pred_train, average=None)\n",
    "\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_train, y_pred_train, average='macro')))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('VALIDATION')\n",
    "y_pred = clf_logreg.predict(X_val)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "scores = f1_score(y_val, y_pred, average=None)\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_val, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST')\n",
    "y_pred = clf_logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('F1: {}'.format(f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dectree = tree.DecisionTreeClassifier(max_depth=10)\n",
    "clf_dectree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names = ['Native', 'B2_0', 'B1_2', 'B1_1', 'A2_0']\n",
    "\n",
    "print('TRAIN')\n",
    "y_pred_train = clf_dectree.predict(X_train)\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "scores = f1_score(y_train, y_pred_train, average=None)\n",
    "\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_train, y_pred_train, average='macro')))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('VALIDATION')\n",
    "y_pred = clf_dectree.predict(X_val)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "scores = f1_score(y_val, y_pred, average=None)\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_val, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST')\n",
    "y_pred = clf_dectree.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "scores = f1_score(y_test, y_pred, average=None)\n",
    "print()\n",
    "print('F1 VALUES')\n",
    "for i in range(len(labels_names)):\n",
    "    print('{0:}:\\t {1:.2f}'.format(labels_names[i], scores[i]))\n",
    "print('Total:\\t {0:.2f}'.format(f1_score(y_test, y_pred, average='macro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
