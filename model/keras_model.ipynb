{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Concatenate, Reshape, LSTM, Dense, Dropout\n",
    "from keras.utils import multi_gpu_model, Sequence\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.append(os.path.abspath('../preprocessing/'))\n",
    "from data_parser import DataParser\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native\n",
      "B1-2\n",
      "B1-1\n",
      "A2-0\n",
      "B2-0\n",
      "(2000,) (2000,)\n",
      "Parsed 2000 samples\n",
      "Shape of data: (2000, 200)\n",
      "Shape of labels: (2000, 5)\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dp = DataParser(Config.embed_dim, Config.max_seq_len)\n",
    "x_train_seq, y_train, x_train_lex, embed_matrix = dp.parse_data()\n",
    "x_train_syn = np.random.randint(0, high=10, size=(int(x_train_seq.shape[0]), int( x_train_seq.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4082)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_lex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lex_input (InputLayer)          (None, 4082)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lex_dense_layer_0 (Dense)       (None, 5)            20415       lex_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 5)            0           lex_dense_layer_0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lex_dense_layer_1 (Dense)       (None, 5)            30          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seq_input (InputLayer)          (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 5)            0           lex_dense_layer_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 200, 50)      510050      seq_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lex_dense_layer_2 (Dense)       (None, 5)            30          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer (LSTM)               (None, 5)            1120        embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 5)            0           lex_dense_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10)           0           lstm_layer[0][0]                 \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat_dense_layer_2 (Dense)    (None, 5)            55          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 5)            0           concat_dense_layer_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 5)            30          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 531,730\n",
      "Trainable params: 21,680\n",
      "Non-trainable params: 510,050\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_input = Input(shape=(Config.max_seq_len,), dtype='int32', name='seq_input')\n",
    "syn_input = Input(shape=(Config.max_seq_len,), dtype='float32', name='syn_input')\n",
    "lex_input = Input(shape=(4082,), dtype='float32', name='lex_input')\n",
    "\n",
    "seq_embeds = Embedding(embed_matrix.shape[0], Config.embed_dim, \n",
    "                       weights=[embed_matrix], input_length=Config.max_seq_len,\n",
    "                       trainable=False,\n",
    "                       embeddings_regularizer=keras.regularizers.l2(.01),\n",
    "                       name='embedding_layer')(seq_input)\n",
    "\n",
    "syn_addons = Reshape((Config.max_seq_len, 1), name='reshape_layer')(syn_input)\n",
    "\n",
    "time_series = seq_embeds\n",
    "# time_series = Concatenate()([seq_embeds, syn_addons])\n",
    "\n",
    "lstm_output = LSTM(Config.lstm_units, \n",
    "                   activation='relu',\n",
    "                   name='lstm_layer')(time_series)\n",
    "\n",
    "lex_feed = lex_input\n",
    "for i in range(3):\n",
    "    lex_feed = Dense(Config.lstm_units,\n",
    "                 activation='relu',\n",
    "                 kernel_regularizer=keras.regularizers.l2(.01),\n",
    "                 name='lex_dense_layer_{}'.format(i))(lex_feed)\n",
    "    lex_feed = Dropout(.01)(lex_feed)\n",
    "\n",
    "concat = Concatenate()([lstm_output, lex_feed])\n",
    "\n",
    "concat_feed = Dense(Config.lstm_units,\n",
    "                    activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(.01),\n",
    "                    name='concat_dense_layer_{}'.format(i))(concat)\n",
    "concat_feed = Dropout(.01)(concat_feed)\n",
    "\n",
    "predictions = Dense(Config.num_classes,\n",
    "                    activation='softmax',\n",
    "                    name='predictions')(concat_feed)\n",
    "\n",
    "model = Model(inputs=[seq_input, syn_input, lex_input], outputs=predictions)\n",
    "\n",
    "sgd = optimizers.SGD(lr=Config.learning_rate, decay=Config.learning_decay)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.3620 - acc: 0.4687 - val_loss: 2102.4748 - val_acc: 0.4100\n",
      "Epoch 2/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.3500 - acc: 0.4687 - val_loss: 2102.4941 - val_acc: 0.3880\n",
      "Epoch 3/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.3451 - acc: 0.4447 - val_loss: 2102.5169 - val_acc: 0.3960\n",
      "Epoch 4/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.3297 - acc: 0.4600 - val_loss: 2102.4581 - val_acc: 0.3980\n",
      "Epoch 5/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.3190 - acc: 0.4627 - val_loss: 2102.4492 - val_acc: 0.4000\n",
      "Epoch 6/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.3010 - acc: 0.4753 - val_loss: 2102.5107 - val_acc: 0.3640\n",
      "Epoch 7/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2913 - acc: 0.4813 - val_loss: 2102.4367 - val_acc: 0.3860\n",
      "Epoch 8/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2811 - acc: 0.4727 - val_loss: 2102.5345 - val_acc: 0.3820\n",
      "Epoch 9/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2664 - acc: 0.4840 - val_loss: 2102.4679 - val_acc: 0.4020\n",
      "Epoch 10/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2663 - acc: 0.4767 - val_loss: 2102.5501 - val_acc: 0.3180\n",
      "Epoch 11/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2510 - acc: 0.4847 - val_loss: 2102.4151 - val_acc: 0.4020\n",
      "Epoch 12/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2581 - acc: 0.4713 - val_loss: 2102.4188 - val_acc: 0.4100\n",
      "Epoch 13/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2360 - acc: 0.4900 - val_loss: 2102.4671 - val_acc: 0.3840\n",
      "Epoch 14/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2184 - acc: 0.4867 - val_loss: 2102.5376 - val_acc: 0.3800\n",
      "Epoch 15/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2271 - acc: 0.4847 - val_loss: 2102.4263 - val_acc: 0.4060\n",
      "Epoch 16/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.2230 - acc: 0.5000 - val_loss: 2102.3990 - val_acc: 0.4000\n",
      "Epoch 17/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.1999 - acc: 0.5027 - val_loss: 2102.3974 - val_acc: 0.4060\n",
      "Epoch 18/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.1815 - acc: 0.5087 - val_loss: 2102.4821 - val_acc: 0.3540\n",
      "Epoch 19/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.1859 - acc: 0.5060 - val_loss: 2102.5643 - val_acc: 0.3840\n",
      "Epoch 20/20\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 2102.1628 - acc: 0.5033 - val_loss: 2102.3869 - val_acc: 0.4020\n",
      "[[303  13  78   3   3]\n",
      " [295   5  91   6   3]\n",
      " [ 69   0 268  46  17]\n",
      " [ 11   0  46 106 237]\n",
      " [  0   0   2  16 382]]\n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "model.fit([x_train_seq, x_train_syn, x_train_lex], y_train, \n",
    "          batch_size=Config.batch_size, \n",
    "          epochs=Config.epochs, \n",
    "          validation_split=0.25,\n",
    "          callbacks=[tensorboard])\n",
    "\n",
    "y_pred_test = model.predict_on_batch([x_train_seq, x_train_syn, x_train_lex])\n",
    "print confusion_matrix(y_train.argmax(axis=-1), y_pred_test.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
