{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Concatenate, Reshape, LSTM, Dense, Dropout\n",
    "from keras.utils import multi_gpu_model, Sequence\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.append(os.path.abspath('../preprocessing/'))\n",
    "from data_parser import DataParser\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataParser(Config.embed_dim, Config.max_seq_len)\n",
    "x_train_seq, x_train_syn, x_train_lex, y_train, embed_matrix = dp.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lex_input (InputLayer)          (None, 4082)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lex_dense_layer_0 (Dense)       (None, 5)            20415       lex_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 5)            0           lex_dense_layer_0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lex_dense_layer_1 (Dense)       (None, 5)            30          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seq_input (InputLayer)          (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 5)            0           lex_dense_layer_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 200, 50)      545700      seq_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lex_dense_layer_2 (Dense)       (None, 5)            30          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer (LSTM)               (None, 10)           2440        embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 5)            0           lex_dense_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 15)           0           lstm_layer[0][0]                 \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat_dense_layer_2 (Dense)    (None, 5)            80          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 5)            0           concat_dense_layer_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 5)            30          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 568,725\n",
      "Trainable params: 23,025\n",
      "Non-trainable params: 545,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_input = Input(shape=(Config.max_seq_len,), dtype='int32', name='seq_input')\n",
    "syn_input = Input(shape=(Config.max_seq_len,), dtype='float32', name='syn_input')\n",
    "lex_input = Input(shape=(4082,), dtype='float32', name='lex_input')\n",
    "\n",
    "seq_embeds = Embedding(embed_matrix.shape[0], Config.embed_dim, \n",
    "                       weights=[embed_matrix], input_length=Config.max_seq_len,\n",
    "                       trainable=False,\n",
    "                       embeddings_regularizer=keras.regularizers.l2(.01),\n",
    "                       name='embedding_layer')(seq_input)\n",
    "\n",
    "# syn_addons = Reshape((Config.max_seq_len, 1), name='reshape_layer')(syn_input)\n",
    "\n",
    "time_series = seq_embeds\n",
    "# time_series = Concatenate()([seq_embeds, syn_addons])\n",
    "\n",
    "lstm_output = LSTM(Config.lstm_units, \n",
    "                   activation='relu',\n",
    "                   name='lstm_layer')(time_series)\n",
    "\n",
    "lex_feed = lex_input\n",
    "for i in range(3):\n",
    "    lex_feed = Dense(Config.lstm_units,\n",
    "                 activation='relu',\n",
    "                 kernel_regularizer=keras.regularizers.l2(.01),\n",
    "                 name='lex_dense_layer_{}'.format(i))(lex_feed)\n",
    "    lex_feed = Dropout(.01)(lex_feed)\n",
    "\n",
    "concat = Concatenate()([lstm_output, lex_feed])\n",
    "\n",
    "concat_feed = Dense(Config.lstm_units,\n",
    "                    activation='relu',\n",
    "                    kernel_regularizer=keras.regularizers.l2(.01),\n",
    "                    name='concat_dense_layer_{}'.format(i))(concat)\n",
    "concat_feed = Dropout(.01)(concat_feed)\n",
    "\n",
    "predictions = Dense(Config.num_classes,\n",
    "                    activation='softmax',\n",
    "                    name='predictions')(concat_feed)\n",
    "\n",
    "model = Model(inputs=[seq_input, syn_input, lex_input], outputs=predictions)\n",
    "\n",
    "sgd = optimizers.SGD(lr=Config.learning_rate, decay=Config.learning_decay)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: nan - acc: 0.1919 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.1944 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.1925 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.2000 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.1969 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.1938 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.1950 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: nan - acc: 0.1944 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.1938 - val_loss: nan - val_acc: 0.1850\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: nan - acc: 0.1956 - val_loss: nan - val_acc: 0.1850\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      " 256/1600 [===>..........................] - ETA: 10s - loss: nan - acc: 0.1680"
     ]
    }
   ],
   "source": [
    "run = 'fine'\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs/{}'.format(run), histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "\n",
    "for i in range(5):\n",
    "    model.fit([x_train_seq, x_train_syn, x_train_lex], y_train, \n",
    "              batch_size=Config.batch_size, \n",
    "              epochs=Config.epochs, \n",
    "              validation_split=0.20,\n",
    "              callbacks=[tensorboard])\n",
    "    model.save('./models/{}_{}.h5'.format(run, i))\n",
    "\n",
    "y_pred_test = model.predict_on_batch([x_train_seq, x_train_syn, x_train_lex])\n",
    "print confusion_matrix(y_train.argmax(axis=-1), y_pred_test.argmax(axis=-1))\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
